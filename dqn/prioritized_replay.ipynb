{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's play around with prioritized replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import math\n",
    "import random\n",
    "from collections import namedtuple\n",
    "import heapq\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "device = torch.device(\"mps\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Blackjack-v1\") # is the stochasticity going to help in highlighting the usefulness of a distributional approach? let's hope so..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN, Replay Memory and Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 128)\n",
    "        self.layer2 = nn.Linear(128, 64)\n",
    "        self.layer3 = nn.Linear(64, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/rl/dqn/replay_buffer.py\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity, alpha, obs_shape, action_shape):\n",
    "        self.capacity = capacity\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.priority_sum = torch.zeros(2 * self.capacity)\n",
    "        self.priority_min = torch.full((2 * self.capacity,), float('inf'))\n",
    "\n",
    "        self.max_priority = 1.\n",
    "\n",
    "        self.data = {\n",
    "            'state': torch.zeros((capacity, *obs_shape)),\n",
    "            'action': torch.zeros((capacity, *action_shape), dtype=torch.int32),\n",
    "            'reward': torch.zeros(capacity, dtype=torch.float32),\n",
    "            'next_state': torch.zeros((capacity, *obs_shape)),\n",
    "            'done': torch.zeros(capacity, dtype=torch.bool)\n",
    "        }\n",
    "        self.next_idx = 0\n",
    "\n",
    "        self.size = 0\n",
    "\n",
    "    def add(self, obs, action, reward, next_obs, done):\n",
    "        idx = self.next_idx\n",
    "\n",
    "        self.data['state'][idx] = obs\n",
    "        self.data['action'][idx] = action\n",
    "        self.data['reward'][idx] = reward\n",
    "        self.data['done'][idx] = done\n",
    "\n",
    "        if next_obs is None:\n",
    "            self.data['next_state'][idx] = torch.zeros_like(next_obs)\n",
    "        else:\n",
    "            self.data['next_state'][idx] = next_obs\n",
    "\n",
    "        self.next_idx = (idx + 1) % self.capacity\n",
    "        self.size = min(self.capacity, self.size + 1)\n",
    "\n",
    "        priority_alpha = self.max_priority ** self.alpha\n",
    "        self._set_priority_min(idx, priority_alpha)\n",
    "        self._set_priority_sum(idx, priority_alpha)\n",
    "\n",
    "    def _set_priority_min(self, idx, priority_alpha):\n",
    "        idx += self.capacity\n",
    "        self.priority_min[idx] = priority_alpha\n",
    "\n",
    "        while idx >= 2:\n",
    "            idx //= 2\n",
    "            self.priority_min[idx] = min(self.priority_min[2 * idx], self.priority_min[2 * idx + 1])\n",
    "\n",
    "    def _set_priority_sum(self, idx, priority):\n",
    "        idx += self.capacity\n",
    "        self.priority_sum[idx] = priority\n",
    "\n",
    "        while idx >= 2:\n",
    "            idx //= 2\n",
    "            self.priority_sum[idx] = self.priority_sum[2 * idx] + self.priority_sum[2 * idx + 1]\n",
    "\n",
    "    def _sum(self):\n",
    "        return self.priority_sum[1]\n",
    "\n",
    "    def _min(self):\n",
    "        return self.priority_min[1]\n",
    "\n",
    "    def find_prefix_sum_idx(self, prefix_sum):\n",
    "        idx = 1\n",
    "        while idx < self.capacity:\n",
    "            if self.priority_sum[idx * 2] > prefix_sum:\n",
    "                idx = 2 * idx\n",
    "            else:\n",
    "                prefix_sum -= self.priority_sum[idx * 2]\n",
    "                idx = 2 * idx + 1\n",
    "\n",
    "        return idx - self.capacity\n",
    "\n",
    "    def sample(self, batch_size, beta):\n",
    "            samples = {\n",
    "                'weights': torch.zeros(size=(batch_size,), dtype=torch.float32),\n",
    "                'indexes': torch.zeros(size=(batch_size,), dtype=torch.int32)\n",
    "            }\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                p = random.random() * self._sum()\n",
    "                idx = self.find_prefix_sum_idx(p)\n",
    "                samples['indexes'][i] = idx\n",
    "\n",
    "            prob_min = self._min() / self._sum()\n",
    "            max_weight = (prob_min * self.size) ** (-beta)\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                idx = int(samples['indexes'][i])\n",
    "                prob = self.priority_sum[idx + self.capacity] / self._sum()\n",
    "                weight = (prob * self.size) ** (-beta)\n",
    "                samples['weights'][i] = weight / max_weight\n",
    "\n",
    "            for k, v in self.data.items():\n",
    "                samples[k] = v[samples['indexes'].long()]\n",
    "\n",
    "            return samples\n",
    "\n",
    "    def update_priorities(self, indexes, priorities):\n",
    "        for idx, priority in zip(indexes, priorities):\n",
    "            self.max_priority = max(self.max_priority, priority)\n",
    "\n",
    "            priority_alpha = priority ** self.alpha\n",
    "            self._set_priority_min(idx, priority_alpha)\n",
    "            self._set_priority_sum(idx, priority_alpha)\n",
    "\n",
    "    def is_full(self):\n",
    "        return self.capacity == self.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonScheduler:\n",
    "    def __init__(self, eps_start, eps_end, eps_decay):\n",
    "        self.eps_start = eps_start\n",
    "        self.eps_end = eps_end\n",
    "        self.eps_decay = eps_decay\n",
    "        self.steps_done = 0\n",
    "\n",
    "    def get_epsilon(self):\n",
    "        epsilon = self.eps_end + (self.eps_start - self.eps_end) * math.exp(-1. * self.steps_done / self.eps_decay)\n",
    "        self.steps_done += 1\n",
    "        return epsilon\n",
    "\n",
    "    def reset(self):\n",
    "        self.steps_done = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters and initialisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "LR = 0.001\n",
    "TARGET_UPDATE = 10\n",
    "ALPHA = 0.6  # Prioritization level\n",
    "BETA_START = 0.4  # Importance-sampling weight\n",
    "BETA_FRAMES = 10_000  # Number of frames over which beta will be annealed to 1\n",
    "MEMORY_CAPACITY = 10_000\n",
    "\n",
    "n_actions = env.action_space.n\n",
    "n_observations = len(env.reset()[0])\n",
    "\n",
    "policy_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "epsilon_scheduler = EpsilonScheduler(EPS_START, EPS_END, EPS_DECAY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action selection and single optimisation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state, policy_net, eps_scheduler):\n",
    "    sample = random.random()\n",
    "    epsilon = eps_scheduler.get_epsilon()\n",
    "\n",
    "    if sample < epsilon:\n",
    "        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "def optimize_model(memory, batch_size=BATCH_SIZE, beta=BETA_START):\n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "\n",
    "    transitions, weights, indexes = memory.sample(batch_size, BETA_START)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    weights = torch.tensor(weights, dtype=torch.float32)\n",
    "\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    next_state_values = torch.zeros(batch_size)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    \n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1), reduction='none')\n",
    "    loss = (loss * weights).mean()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    new_priorities = torch.abs(state_action_values - expected_state_action_values.unsqueeze(1)).detach().numpy()\n",
    "    memory.update_priorities(indexes, new_priorities + 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, _ = env.reset()\n",
    "state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "action = select_action(state, policy_net, epsilon_scheduler)\n",
    "\n",
    "# init memory here because i need select_action()\n",
    "memory = ReplayMemory(capacity=MEMORY_CAPACITY, alpha=0.6, obs_shape=state.shape, action_shape=action.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't assign a NoneType to a torch.FloatTensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     next_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[43mmemory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[1;32m     19\u001b[0m optimize_model(memory)\n",
      "Cell \u001b[0;32mIn[4], line 28\u001b[0m, in \u001b[0;36mReplayMemory.add\u001b[0;34m(self, obs, action, reward, next_obs, done)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate\u001b[39m\u001b[38;5;124m'\u001b[39m][idx] \u001b[38;5;241m=\u001b[39m obs\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maction\u001b[39m\u001b[38;5;124m'\u001b[39m][idx] \u001b[38;5;241m=\u001b[39m action\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreward\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdone\u001b[39m\u001b[38;5;124m'\u001b[39m][idx] \u001b[38;5;241m=\u001b[39m done\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m next_obs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: can't assign a NoneType to a torch.FloatTensor"
     ]
    }
   ],
   "source": [
    "num_episodes = 100\n",
    "for episode in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    state = torch.tensor([state], dtype=torch.float32)\n",
    "\n",
    "    while True:\n",
    "        action = select_action(state, policy_net, epsilon_scheduler)\n",
    "        next_state, reward, done, _, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], dtype=torch.float32)\n",
    "\n",
    "        if not done:\n",
    "            next_state = torch.tensor([next_state], dtype=torch.float32)\n",
    "        else:\n",
    "            next_state = None\n",
    "\n",
    "        # TODO something is wrong with next state handling here...\n",
    "        memory.add(state, action, next_state, reward, done)\n",
    "        state = next_state\n",
    "\n",
    "        optimize_model(memory)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    if episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(env, num_episodes, device, policy_net=None):\n",
    "    agent = \"Random\" if policy_net is None else \"Trained\"\n",
    "    if policy_net is None:\n",
    "        action_selection = lambda state: torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)\n",
    "    else:\n",
    "        action_selection = lambda state: policy_net(state).max(1)[1].view(1, 1)\n",
    "\n",
    "    wins = 0\n",
    "    total_earnings = 0\n",
    "    for _ in range(num_episodes):\n",
    "        state, info = env.reset()\n",
    "        state = torch.tensor([state], dtype=torch.float32, device=device)\n",
    "\n",
    "        while True:\n",
    "            with torch.no_grad():\n",
    "                action = action_selection(state)\n",
    "            next_state, reward, done, _, _ = env.step(action.item())\n",
    "            total_earnings += reward\n",
    "\n",
    "            state = torch.tensor([next_state], dtype=torch.float32, device=device)\n",
    "\n",
    "            if done:\n",
    "                if reward > 0:\n",
    "                    wins += 1\n",
    "                break\n",
    "\n",
    "    win_rate = wins / num_episodes\n",
    "    average_earnings = total_earnings / num_episodes\n",
    "    print(f'{agent} agent => Evaluation over {num_episodes} episodes. Win Rate: {win_rate:.2f}; Avg. Earnings: {average_earnings:.2f}')\n",
    "\n",
    "    return win_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained agent => Evaluation over 1000 episodes. Win Rate: 0.00; Avg. Earnings: -1.00\n",
      "Random agent => Evaluation over 1000 episodes. Win Rate: 0.28; Avg. Earnings: -0.39\n"
     ]
    }
   ],
   "source": [
    "num_evaluation_episodes = 1000\n",
    "evaluate_agent(env, num_evaluation_episodes, device, policy_net=policy_net);\n",
    "evaluate_agent(env, num_evaluation_episodes, device);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not too shabby."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rei",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
