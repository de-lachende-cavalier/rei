{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments with two agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import Video\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bitmap_with_two_connected_goals(show=True):\n",
    "    # Start with an all-zero 8x8 bitmap (black grid)\n",
    "    bitmap = np.zeros((8, 8), dtype=int)\n",
    "\n",
    "    # Randomly select a position for the first goal (white pixel)\n",
    "    goal_position_1 = np.random.randint(0, 8, 2)\n",
    "    bitmap[goal_position_1[0], goal_position_1[1]] = 1\n",
    "\n",
    "    # Define the possible positions for the second goal in the Moore neighborhood\n",
    "    neighbors = [\n",
    "        (-1, 0), (1, 0), (0, -1), (0, 1), \n",
    "        (-1, -1), (-1, 1), (1, -1), (1, 1)  # Including diagonals\n",
    "    ]\n",
    "\n",
    "    possible_positions = [\n",
    "        (goal_position_1[0] + dx, goal_position_1[1] + dy) \n",
    "        for dx, dy in neighbors\n",
    "        if 0 <= goal_position_1[0] + dx < 8 and 0 <= goal_position_1[1] + dy < 8\n",
    "    ]\n",
    "\n",
    "    # Randomly select one of these positions for the second goal\n",
    "    goal_position_2 = possible_positions[np.random.randint(len(possible_positions))]\n",
    "    bitmap[goal_position_2[0], goal_position_2[1]] = 1\n",
    "\n",
    "    if show:\n",
    "        plt.imshow(bitmap)\n",
    "        plt.axis('off')\n",
    "        plt.show\n",
    "\n",
    "    return bitmap, tuple(goal_position_1), goal_position_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 1, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 1, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " (3, 6),\n",
       " (2, 6))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAFI0lEQVR4nO3XsWkDURRFwZXYKlTFNmFUgat0BUJNqAqV4a/sRAYbw/IFOxO/4GaHdxpjjAUAlmU5zx4AwPsQBQAiCgBEFACIKAAQUQAgogBARAGArH89/Dh/7rkDgJ3dv79+vfEpABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQNbZA4Bjuj0fsyf82/WyzZ6wG58CABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAyDp7AHBM18s2ewI/8CkAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFADIaYwxZo8A4D34FACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAyAtkmRGYLZnfIgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_bitmap_with_two_connected_goals()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Romeo and Juliet (with relative positions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to train two agents to run towards each other as quickly as possible, no matter their starting positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 0), (7, 4)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAFL0lEQVR4nO3XsU3DYBRGUSfyFJ7CSyBPwJRMELFEpsgY/HS3xUKybNA59Su+7urdxhhjAoBpmu5nDwDgOkQBgIgCABEFACIKAEQUAIgoABBRACDz3sO3+/uROwA42OfXx483PgUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgCZ9x4+Xs8DZxxnW9azJwD8GT4FACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIPPew21ZD5wBwBX4FACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEDmswfAVTxez7Mn/Mq2rGdP4B/xKQAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgC5jTHG2SMAuAafAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIA+QaPtRSadGikKgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def spawn_agents_in_grid(grid_size=(8, 8), num_agents=2, show=False):\n",
    "    agent_positions = []\n",
    "\n",
    "    while len(agent_positions) < num_agents:\n",
    "        position = tuple(np.random.randint(0, grid_size[0], 2))\n",
    "        bitmap = np.zeros((8, 8), dtype=int)\n",
    "\n",
    "        # Ensure the position is unique\n",
    "        if position not in agent_positions:\n",
    "            agent_positions.append(position)\n",
    "\n",
    "    for pos in agent_positions:\n",
    "        bitmap[pos[0]][pos[1]] = 1\n",
    "\n",
    "    if show:\n",
    "        plt.imshow(bitmap)\n",
    "        plt.axis('off')\n",
    "        plt.show\n",
    "\n",
    "    return bitmap, agent_positions\n",
    "\n",
    "_, agent_positions = spawn_agents_in_grid(show=True)\n",
    "agent_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSAAgent:\n",
    "    def __init__(self, alpha=0.1, gamma=0.9, epsilon=0.1, epsilon_decay=0.995, epsilon_min=0.01):\n",
    "        self.q_table = np.zeros((8, 8, 8, 8, 9))  # 8x8 grid, 8x8 relative positions, and 9 possible actions\n",
    "        self.alpha = alpha  # Learning rate\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.epsilon = epsilon  # Exploration rate\n",
    "        self.epsilon_decay = epsilon_decay  # Decay rate of exploration\n",
    "        self.epsilon_min = epsilon_min  # Minimum exploration rate\n",
    "        \n",
    "        # Define the action space\n",
    "        self.actions = ['up', 'down', 'left', 'right', 'up_left', 'up_right', 'down_left', 'down_right', 'stay']\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            # Explore: choose a random action\n",
    "            return np.random.choice(self.actions)\n",
    "        else:\n",
    "            # Exploit: choose the best action from Q-table\n",
    "            x, y, dx, dy = state\n",
    "            return self.actions[np.argmax(self.q_table[x, y, dx, dy])]\n",
    "\n",
    "    def get_next_state(self, current_state, action):\n",
    "        # This function returns the next state based on the current state and action\n",
    "        x, y = current_state\n",
    "        if action == 'up':\n",
    "            return (max(x-1, 0), y)\n",
    "        elif action == 'down':\n",
    "            return (min(x+1, 7), y)\n",
    "        elif action == 'left':\n",
    "            return (x, max(y-1, 0))\n",
    "        elif action == 'right':\n",
    "            return (x, min(y+1, 7))\n",
    "        elif action == 'up_left':\n",
    "            return (max(x-1, 0), max(y-1, 0))\n",
    "        elif action == 'up_right':\n",
    "            return (max(x-1, 0), min(y+1, 7))\n",
    "        elif action == 'down_left':\n",
    "            return (min(x+1, 7), max(y-1, 0))\n",
    "        elif action == 'down_right':\n",
    "            return (min(x+1, 7), min(y+1, 7))\n",
    "        else:\n",
    "            return current_state\n",
    "\n",
    "    def action_index(self, action):\n",
    "        return self.actions.index(action)\n",
    "\n",
    "    def update(self, current_state, action, reward, next_state, next_action):\n",
    "        # Convert actions to their index in the Q-table\n",
    "        action_index = self.action_index(action)\n",
    "        next_action_index = self.action_index(next_action)\n",
    "\n",
    "        # Perform the SARSA update to the Q-table\n",
    "        current_q = self.q_table[current_state + (action_index,)]\n",
    "        next_q = self.q_table[next_state + (next_action_index,)]\n",
    "        target_q = reward + self.gamma * next_q\n",
    "        self.q_table[current_state + (action_index,)] += self.alpha * (target_q - current_q)\n",
    "\n",
    "        # Decay epsilon\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_relative_position(agent_pos, other_agent_pos):\n",
    "    dx = other_agent_pos[0] - agent_pos[0]\n",
    "    dy = other_agent_pos[1] - agent_pos[1]\n",
    "    return (dx, dy)\n",
    "\n",
    "def is_in_moore_neighborhood(pos1, pos2):\n",
    "    \"\"\"Check if two positions are in each other's Moore neighborhood.\"\"\"\n",
    "    dx = abs(pos1[0] - pos2[0])\n",
    "    dy = abs(pos1[1] - pos2[1])\n",
    "    return dx <= 1 and dy <= 1\n",
    "\n",
    "def train_agents(agent1, agent2, num_episodes=1000, num_steps=100):\n",
    "    for episode in range(num_episodes):\n",
    "        # Initialize the positions of both agents\n",
    "        _, positions = spawn_agents_in_grid()\n",
    "        \n",
    "        # pos of 2 with respect to 1\n",
    "        relative_pos2 = calculate_relative_position(positions[0], positions[1])\n",
    "        # pos of 1 with respect to 2\n",
    "        relative_pos1 = calculate_relative_position(positions[1], positions[0])\n",
    "\n",
    "        state1 = positions[0] + relative_pos2\n",
    "        state2 = positions[1] + relative_pos1\n",
    "\n",
    "        action1 = agent1.choose_action(state1)\n",
    "        action2 = agent2.choose_action(state2)\n",
    "\n",
    "        for step in range(num_steps):  # Limit the number of steps per episode\n",
    "            next_position1 = agent1.get_next_state(positions[0], action1)\n",
    "            next_position2 = agent2.get_next_state(positions[1], action2)\n",
    "\n",
    "            next_relative_pos2 = calculate_relative_position(next_position1, next_position2)\n",
    "            next_relative_pos1 = calculate_relative_position(next_position2, next_position1)\n",
    "\n",
    "            # check if agents are in each other's Moore neighborhood\n",
    "            reward = 0 if is_in_moore_neighborhood(next_position1, next_position2) else -1\n",
    "\n",
    "            next_state1 = next_position1 + next_relative_pos2\n",
    "            next_state2 = next_position2 + next_relative_pos1\n",
    "\n",
    "            next_action1 = agent1.choose_action(next_state1)\n",
    "            next_action2 = agent2.choose_action(next_state2)\n",
    "\n",
    "            agent1.update(state1, action1, reward, next_state1, next_action1)\n",
    "            agent2.update(state2, action2, reward, next_state2, next_action2)\n",
    "\n",
    "            positions = (next_position1, next_position2)\n",
    "            relative_pos1, relative_pos2 = next_relative_pos1, next_relative_pos2\n",
    "            action1, action2 = next_action1, next_action2\n",
    "\n",
    "            if reward == 0:\n",
    "                break  # Agents are in each other's Moore neighborhood, end the episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "romeo = SARSAAgent()\n",
    "juliet = SARSAAgent()\n",
    "\n",
    "train_agents(romeo, juliet, num_episodes=100_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_video(agent1, agent2, filename='two_agents_movement.mp4'):\n",
    "    grid, positions = spawn_agents_in_grid()\n",
    "    # Set up the video writer with a higher FPS to slow down the movement\n",
    "    writer = imageio.get_writer(\"videos/\" + filename, fps=3)\n",
    "\n",
    "    # Increase the size of the grid for better visibility\n",
    "    scale_factor = 50  # Increase this if you want even larger frames\n",
    "    large_grid = np.kron(grid, np.ones((scale_factor, scale_factor)))\n",
    "\n",
    "    for step in range(100):  # Assume a maximum of 100 steps\n",
    "        # Update the grid with the current agent position (set to 1 for white)\n",
    "        grid[positions[0]] = 1\n",
    "        grid[positions[1]] = 1\n",
    "        large_grid = np.kron(grid, np.ones((scale_factor, scale_factor)))  # Scale up\n",
    "        frame = (large_grid * 255).astype(np.uint8)  # Convert to an image\n",
    "        frame = np.stack((frame,) * 3, axis=-1)  # Convert to RGB\n",
    "\n",
    "        # Repeat each frame three times\n",
    "        for _ in range(3):\n",
    "            writer.append_data(frame)\n",
    "\n",
    "        # Set the agent's previous position back to 0 (black)\n",
    "        grid[positions[0]] = 0\n",
    "        grid[positions[1]] = 0\n",
    "\n",
    "        relative_pos2 = calculate_relative_position(positions[0], positions[1])\n",
    "        # pos of 1 with respect to 2\n",
    "        relative_pos1 = calculate_relative_position(positions[1], positions[0])\n",
    "\n",
    "        # Move the agent\n",
    "        action1 = agent1.choose_action(positions[0] + relative_pos2)\n",
    "        action2 = agent2.choose_action(positions[1] + relative_pos1)\n",
    "        next_position1 = agent1.get_next_state(positions[0], action1)\n",
    "        next_position2 = agent2.get_next_state(positions[1], action2)\n",
    "\n",
    "        # If the agent reaches the goal, update the grid and append the final frames\n",
    "        if is_in_moore_neighborhood(next_position1, next_position2):\n",
    "            grid[next_position1] = 1\n",
    "            grid[next_position2] = 1\n",
    "            large_grid = np.kron(grid, np.ones((scale_factor, scale_factor)))\n",
    "            final_frame = (large_grid * 255).astype(np.uint8)\n",
    "            final_frame = np.stack((final_frame,) * 3, axis=-1)\n",
    "            for _ in range(3):\n",
    "                writer.append_data(final_frame)\n",
    "            break  # Break after showing the final move\n",
    "\n",
    "        # Update the start position for the next step\n",
    "        positions[0] = next_position1\n",
    "        positions[1] = next_position2\n",
    "\n",
    "    writer.close()  # Close the writer to finalize the video\n",
    "\n",
    "    return \"videos/\" + filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"videos/two_untrained.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video(make_video(SARSAAgent(), SARSAAgent(), filename=\"two_untrained.mp4\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"videos/two_trained.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video(make_video(romeo, juliet, filename=\"two_trained.mp4\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, after training for 100x longer than in the single agent case, we get something that looks quite good, even though (1) the agents move apart before coming closer and (2) this might very well be a stochastic fluke..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A more robust testing environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline\n",
    "untrained1, untrained2 = SARSAAgent(), SARSAAgent()\n",
    "\n",
    "# trained ones\n",
    "trained1, trained2 = SARSAAgent(), SARSAAgent()\n",
    "train_agents(trained1, trained2, num_episodes=100_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_trained_better(trained1, trained2, untrained1, untrained2, m):\n",
    "    steps_untrained = np.zeros(m)\n",
    "    steps_trained = np.zeros(m)\n",
    "\n",
    "    # increase m for higher statistical significance\n",
    "    for i in range(m):\n",
    "        _, positions = spawn_agents_in_grid()\n",
    "        \n",
    "        positions_t = positions # trained\n",
    "        positions_u = positions # untrained\n",
    "        for stepn in range(100):\n",
    "            #trained\n",
    "            relative_pos2_t = calculate_relative_position(positions_t[0], positions_t[1])\n",
    "            relative_pos1_t = calculate_relative_position(positions_t[1], positions_t[0])\n",
    "            relative_pos2_u = calculate_relative_position(positions_u[0], positions_u[1])\n",
    "            relative_pos1_u = calculate_relative_position(positions_u[1], positions_u[0])\n",
    "\n",
    "            action1_t = trained1.choose_action(positions_t[0] + relative_pos2_t)\n",
    "            action2_t = trained2.choose_action(positions_t[1] + relative_pos1_t)\n",
    "            action1_u = untrained1.choose_action(positions_u[0] + relative_pos2_u)\n",
    "            action2_u = untrained2.choose_action(positions_u[1] + relative_pos1_u)\n",
    "\n",
    "            next_position1_t = trained1.get_next_state(positions_t[0], action1_t)\n",
    "            next_position2_t = trained2.get_next_state(positions_t[1], action2_t)\n",
    "            next_position1_u = untrained1.get_next_state(positions_u[0], action1_u)\n",
    "            next_position2_u = untrained2.get_next_state(positions_u[1], action2_u)\n",
    "\n",
    "            # if the trained agents reach the goal => append steps, add one more and append to untrained as well, so that it's greater than\n",
    "            if is_in_moore_neighborhood(next_position1_t, next_position2_t):\n",
    "                steps_trained[i] = stepn\n",
    "                steps_untrained[i] = stepn + 10\n",
    "                break\n",
    "\n",
    "            # as above, in the case the untrained agents reach the goal quicker (this should happen very rarely!)\n",
    "            if is_in_moore_neighborhood(next_position1_u, next_position2_u):\n",
    "                steps_untrained[i] = stepn\n",
    "                steps_trained[i] = stepn + 10\n",
    "                break\n",
    "            \n",
    "            positions_t[0] = next_position1_t\n",
    "            positions_t[1] = next_position2_t\n",
    "\n",
    "            positions_u[0] = next_position1_u\n",
    "            positions_u[1] = next_position2_u\n",
    "\n",
    "    return steps_trained, steps_untrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 1000\n",
    "trained, untrained = check_trained_better(trained1, trained2, untrained1, untrained2, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False,  True, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "        True, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_trained_worse = trained > untrained\n",
    "is_trained_worse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.998"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_trained_better = is_trained_worse.size - np.count_nonzero(is_trained_worse)\n",
    "\n",
    "n_trained_better / m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that's a plaesant surprise! The trained model is better >99% of the time! Even though we're using simple SARSA, even though the problem is non-stationary.\n",
    "\n",
    "Naturally, this happens, in great part, because of how simple the environment is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "relsos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
